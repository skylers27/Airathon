{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "199dba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import Flatten, Dense\n",
    "import dateutil\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "340e885a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   datetime grid_id       value\n",
      "0      2018-02-01T08:00:00Z   3S31A   11.400000\n",
      "1      2018-02-01T08:00:00Z   A2FBI   17.000000\n",
      "2      2018-02-01T08:00:00Z   DJN0F   11.100000\n",
      "3      2018-02-01T08:00:00Z   E5P9N   22.100000\n",
      "4      2018-02-01T08:00:00Z   FRITQ   29.800000\n",
      "...                     ...     ...         ...\n",
      "34307  2020-12-31T18:30:00Z   P8JA5  368.611111\n",
      "34308  2020-12-31T18:30:00Z   PW0JT  294.425000\n",
      "34309  2020-12-31T18:30:00Z   VXNN3  224.857143\n",
      "34310  2020-12-31T18:30:00Z   VYH7U  287.000000\n",
      "34311  2020-12-31T18:30:00Z   ZF3ZW  410.500000\n",
      "\n",
      "[34312 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "train_labels = pd.read_csv(\"train_labels.csv\")\n",
    "grid_metadata = pd.read_csv(\"grid_metadata.csv\")\n",
    "satellite_metadata = pd.read_csv(\"satellite_metadata.csv\")\n",
    "satellite_metadata['Date'] =  pd.to_datetime(satellite_metadata['time_end'], format='%Y-%m-%d')\n",
    "\n",
    "print(train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9b7fbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels = train_labels.sample(1000, random_state=42)\n",
    "train_labels = train_labels.sample(3000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc868f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_grid_data(metadata, grid_id):\n",
    "    return metadata[metadata[\"grid_id\"] == grid_id]\n",
    "\n",
    "def fetch_satellite_meta(metadata, datetime, location, datatype, split):\n",
    "    if location == \"Delhi\":\n",
    "        location = \"dl\"\n",
    "    elif location == \"Taipei\":\n",
    "        location = \"tpe\"\n",
    "    else:\n",
    "        location = \"la\"\n",
    "\n",
    "    metadata = metadata[metadata['location'] == location]\n",
    "    metadata = metadata[metadata['product'] == datatype]\n",
    "    metadata = metadata[metadata['split'] == split]\n",
    "    dateobject = dateutil.parser.parse(datetime)\n",
    "    return metadata.loc[(metadata['Date'].dt.month == dateobject.month) & \n",
    "                        (metadata['Date'].dt.day == dateobject.day) &\n",
    "                        (metadata['Date'].dt.year <= dateobject.year)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8e41e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Opens the HDF file\n",
    "def load_data(FILEPATH):\n",
    "    ds = gdal.Open(FILEPATH)\n",
    "    return ds\n",
    "\n",
    "def fetch_subset(granule_id):\n",
    "    ds = load_data(\"dataset/\" + granule_id)\n",
    "    ds.GetSubDatasets()[0]\n",
    "    raster = gdal.Open(ds.GetSubDatasets()[8][0]) #grid5km:cosSZA features only\n",
    "    band = raster.GetRasterBand(1)\n",
    "    band_arr = band.ReadAsArray()\n",
    "    return band_arr\n",
    "\n",
    "\n",
    "def fetch_training_features(grid_id, datetime, split):\n",
    "    temp = get_grid_data(grid_metadata, grid_id)\n",
    "    sat_met = fetch_satellite_meta(satellite_metadata, \n",
    "                               datetime, \n",
    "                               temp.iloc[0]['location'], \n",
    "                               \"maiac\", \n",
    "                               split)\n",
    "    counter = 0\n",
    "    features = None\n",
    "    for i in range(len(sat_met)):\n",
    "        counter+=1\n",
    "        granule_id = sat_met.iloc[i]['granule_id']\n",
    "        subset = fetch_subset(granule_id)\n",
    "        if features is None:\n",
    "            features = subset\n",
    "        else:\n",
    "            features+=subset\n",
    "    return features/counter\n",
    "\n",
    "def generate_features(train_labels, split):\n",
    "    labels = []\n",
    "    features = []\n",
    "    for i in range(len(train_labels)):\n",
    "        if i % 500 == 0: print(i)\n",
    "        feature = fetch_training_features(train_labels.iloc[i]['grid_id'], train_labels.iloc[i]['datetime'], split)\n",
    "        features.append(np.array(feature).reshape(-1))\n",
    "        if split == \"train\":\n",
    "            labels.append(train_labels.iloc[i]['value'])\n",
    "    return np.array(features), np.array(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39b1e904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features, labels = generate_features(train_labels, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ee0c06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100, 240, 240)\n",
      "(900, 240, 240)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels,  test_size=0.30, random_state=42)\n",
    "\n",
    "shape_tuple = (240,240)\n",
    "X_train_reshape = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train_reshape.append(X_train[i].reshape(shape_tuple))\n",
    "X_train_reshape = np.stack(X_train_reshape, axis=0 )\n",
    "print(X_train_reshape.shape)\n",
    "\n",
    "X_test_reshape = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test_reshape.append(X_test[i].reshape(shape_tuple))\n",
    "X_test_reshape = np.stack(X_test_reshape, axis=0 )\n",
    "print(X_test_reshape.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "295c6e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train_reshape[i] = normalize(X_train_reshape[i])\n",
    "    \n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test_reshape[i] = normalize(X_test_reshape[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45d043de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train_reshape)\n",
    "X_test = torch.from_numpy(X_test_reshape)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_train = torch.unsqueeze(y_train,dim=1)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "y_test = torch.unsqueeze(y_test,dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae884bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[-0.7204, -1.1817, -2.3289, -0.6339,  0.3079],\n",
      "        [ 0.9281, -1.2470, -0.4750,  0.0217,  0.3422],\n",
      "        [ 0.3498,  0.1529, -1.3565,  0.2168,  0.5467]], requires_grad=True)\n",
      "target:  tensor([[ 0.9591,  2.1842, -0.6572, -0.7069,  0.1278],\n",
      "        [ 0.2686, -0.1737,  0.6260, -1.6006, -2.0905],\n",
      "        [-1.5156, -2.4388,  0.3009,  1.7885,  0.1222]])\n",
      "output:  tensor(2.9284, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "mse_loss = nn.MSELoss()\n",
    "output = mse_loss(input, target)\n",
    "output.backward()\n",
    "\n",
    "print('input: ', input)\n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0a45e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=3,kernel_size=5, padding=0) #3,6,5\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(3, 16, 5)\n",
    "        # self.fc1 = nn.Linear(16 * 5 * 5, 84)\n",
    "        self.fc1 = nn.Linear(51984,84)\n",
    "        self.fc2 = nn.Linear(84, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3747ac7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=51984, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cfa4702",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 #unused\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.00001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7119f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16028\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:528: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "[2,  2000] loss: 6178.205\n",
      "2000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(2):  # loop over the dataset multiple times\n",
    "    # running_loss = 0.0\n",
    "epoch = 1\n",
    "running_loss = 0\n",
    "for i in range(X_train.shape[0]):\n",
    "    if i % 200 == 0: print(i)\n",
    "    # data[i].unsqueeze(0)\n",
    "    inputs = X_train[i].unsqueeze(0).unsqueeze(0)\n",
    "    # print(X_train[i].unsqueeze(0).unsqueeze(0).shape)\n",
    "    inputs = inputs.float()\n",
    "    labels = y_train[i]\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "        print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "        running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13877515",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    preds = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        preds.append(net(X_test[i].unsqueeze(0).unsqueeze(0).float()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381d936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
