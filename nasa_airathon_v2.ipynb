{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d94b0656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Skyler Added:\n",
    "from dateutil import parser\n",
    "from keras.layers import Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ecf21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(\"train_labels.csv\")\n",
    "grid_metadata = pd.read_csv(\"grid_metadata.csv\")\n",
    "satellite_metadata = pd.read_csv(\"satellite_metadata.csv\")\n",
    "satellite_metadata['Date'] =  pd.to_datetime(satellite_metadata['time_end'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec24bb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# REMOVE THIS LINE #\n",
    "####################\n",
    "train_labels = train_labels.sample(5, random_state=42)\n",
    "#print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ceeb22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid_data(metadata, grid_id):\n",
    "    return metadata[metadata[\"grid_id\"] == grid_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6821e526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_satellite_meta(metadata, datetime, location, datatype, split):\n",
    "    if location == \"Delhi\":\n",
    "        location = \"dl\"\n",
    "    elif location == \"Taipei\":\n",
    "        location = \"tpe\"\n",
    "    else:\n",
    "        location = \"la\"\n",
    "        \n",
    "    metadata = metadata[metadata['location'] == location]\n",
    "    metadata = metadata[metadata['product'] == datatype]\n",
    "    metadata = metadata[metadata['split'] == split]\n",
    "    dateobject = parser.parse(datetime)\n",
    "    return metadata.loc[(metadata['Date'].dt.month == dateobject.month) & \n",
    "                        (metadata['Date'].dt.day == dateobject.day) &\n",
    "                        (metadata['Date'].dt.year <= dateobject.year)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9212e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens the HDF file\n",
    "def load_data(FILEPATH):\n",
    "    print(FILEPATH)\n",
    "    ds = gdal.Open(FILEPATH)\n",
    "    return ds\n",
    "\n",
    "def fetch_subset(granule_id):\n",
    "    ds = load_data(\"C:/Users/Skyler/\" + granule_id)\n",
    "    ds.GetSubDatasets()[0]\n",
    "    band_arr = []\n",
    "    for i in range(13): #0-12 valid index\n",
    "        #print(i)\n",
    "        raster = gdal.Open(ds.GetSubDatasets()[i][0]) #grid5km:cosSZA features only\n",
    "        band = raster.GetRasterBand(1)\n",
    "        temp_band_arr = band.ReadAsArray()\n",
    "        \n",
    "        if temp_band_arr.shape == (240,240): #if every 5km, expand to full dimension\n",
    "            temp_band_arr = temp_band_arr.repeat(5, axis=0).repeat(5, axis=1)\n",
    "            \n",
    "        #print(temp_band_arr.size)\n",
    "        band_arr.append(temp_band_arr)\n",
    "    band_arr = np.array(band_arr).flatten()\n",
    "    print(band_arr.shape)\n",
    "    return band_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52aa4365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_training_features(grid_id, datetime, split):\n",
    "    temp = get_grid_data(grid_metadata, grid_id)\n",
    "    sat_met = fetch_satellite_meta(satellite_metadata, \n",
    "                               datetime, \n",
    "                               temp.iloc[0]['location'], \n",
    "                               \"maiac\", \n",
    "                               split)\n",
    "    counter = 0\n",
    "    features = None\n",
    "    for i in range(len(sat_met)):\n",
    "        counter+=1\n",
    "        #Lines below added\n",
    "        split = str(sat_met.iloc[i]['split'])\n",
    "        product = str(sat_met.iloc[i]['product'])\n",
    "        year = str(sat_met.iloc[i]['Date'].year)\n",
    "        granule_id = str(sat_met.iloc[i]['granule_id'])\n",
    "        path_str = split + \"/\" + product + \"/\" + year + \"/\" + granule_id\n",
    "        \n",
    "        subset = fetch_subset(path_str)\n",
    "        if features is None:\n",
    "            features = subset\n",
    "        else:\n",
    "            features+=subset\n",
    "    return features/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d270d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(train_labels, split):\n",
    "    labels = []\n",
    "    features = []\n",
    "    for i in range(len(train_labels)):\n",
    "        feature = fetch_training_features(train_labels.iloc[i]['grid_id'], train_labels.iloc[i]['datetime'], split)\n",
    "        features.append(np.array(feature).reshape(-1))\n",
    "        if split == \"train\":\n",
    "            labels.append(train_labels.iloc[i]['value'])\n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56ce54d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Skyler/train/maiac/2018/20180514T070000_maiac_dl_0.hdf\n",
      "C:/Users/Skyler/train/maiac/2018/20180514T070000_maiac_dl_0.hdf\n",
      "C:/Users/Skyler/train/maiac/2018/20180514T070000_maiac_dl_0.hdf\n",
      "C:/Users/Skyler/train/maiac/2018/20180514T070000_maiac_dl_0.hdf\n",
      "C:/Users/Skyler/train/maiac/2018/20180514T070000_maiac_dl_0.hdf\n",
      "C:/Users/Skyler/train/maiac/2018/20180514T070000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20190514T063000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2020/20200514T064500_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20190514T063000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2020/20200514T064500_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20190514T063000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2020/20200514T064500_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20190514T063000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2020/20200514T064500_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20190514T063000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2020/20200514T064500_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20190514T063000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2020/20200514T064500_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "(18720000,)\n",
      "(18720000,)\n",
      "(18720000,)\n",
      "(18720000,)\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2018/20180604T054500_maiac_dl_0.hdf\n",
      "C:/Users/Skyler/train/maiac/2018/20180604T054500_maiac_dl_0.hdf\n",
      "C:/Users/Skyler/train/maiac/2018/20180604T054500_maiac_dl_0.hdf\n",
      "C:/Users/Skyler/train/maiac/2018/20180604T054500_maiac_dl_0.hdf\n",
      "C:/Users/Skyler/train/maiac/2018/20180604T054500_maiac_dl_0.hdf\n",
      "C:/Users/Skyler/train/maiac/2018/20180604T054500_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20190604T065000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20190604T065000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20190604T065000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20190604T065000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20190604T065000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20190604T065000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2020/20200604T070000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2020/20200604T070000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2020/20200604T070000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2020/20200604T070000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2020/20200604T070000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2020/20200604T070000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2018/20180604T202500_maiac_la_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2018/20180604T202500_maiac_la_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2018/20180604T202500_maiac_la_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2018/20180604T202500_maiac_la_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2018/20180604T202500_maiac_la_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2018/20180604T202500_maiac_la_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2018/20181225T191500_maiac_la_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2018/20181225T191500_maiac_la_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2018/20181225T191500_maiac_la_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2018/20181225T191500_maiac_la_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2018/20181225T191500_maiac_la_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2018/20181225T191500_maiac_la_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20191225T202000_maiac_la_0.hdf\n",
      "(18720000,)\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20191225T202000_maiac_la_0.hdf\n",
      "(18720000,)\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20191225T202000_maiac_la_0.hdf\n",
      "(18720000,)\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20191225T202000_maiac_la_0.hdf\n",
      "(18720000,)\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20191225T202000_maiac_la_0.hdf\n",
      "(18720000,)\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20191225T202000_maiac_la_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2018/20181018T063000_maiac_dl_0.hdf\n",
      "C:/Users/Skyler/train/maiac/2018/20181018T063000_maiac_dl_0.hdf\n",
      "C:/Users/Skyler/train/maiac/2018/20181018T063000_maiac_dl_0.hdf\n",
      "C:/Users/Skyler/train/maiac/2018/20181018T063000_maiac_dl_0.hdf\n",
      "C:/Users/Skyler/train/maiac/2018/20181018T063000_maiac_dl_0.hdf\n",
      "C:/Users/Skyler/train/maiac/2018/20181018T063000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20191018T060000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20191018T060000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20191018T060000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20191018T060000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20191018T060000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2019/20191018T060000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2020/20201018T061000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2020/20201018T061000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2020/20201018T061000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2020/20201018T061000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2020/20201018T061000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "C:/Users/Skyler/train/maiac/2020/20201018T061000_maiac_dl_0.hdf\n",
      "(18720000,)\n",
      "(18720000,)\n",
      "(18720000,)\n",
      "(18720000,)\n",
      "(18720000,)\n",
      "(18720000,)\n"
     ]
    }
   ],
   "source": [
    "features, labels = generate_features(train_labels, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5483a395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Skyler\\AppData\\Local\\Temp/ipykernel_7616/353860434.py:18: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  estimator = KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=5, verbose=0)\n",
      "C:\\Users\\Skyler\\AppData\\Local\\Temp/ipykernel_7616/353860434.py:18: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  estimator = KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=5, verbose=0)\n",
      "C:\\Users\\Skyler\\AppData\\Local\\Temp/ipykernel_7616/353860434.py:18: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  estimator = KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=5, verbose=0)\n",
      "C:\\Users\\Skyler\\AppData\\Local\\Temp/ipykernel_7616/353860434.py:18: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  estimator = KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=5, verbose=0)\n",
      "C:\\Users\\Skyler\\AppData\\Local\\Temp/ipykernel_7616/353860434.py:18: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  estimator = KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=5, verbose=0)\n",
      "C:\\Users\\Skyler\\AppData\\Local\\Temp/ipykernel_7616/353860434.py:18: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  estimator = KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=5, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x000001852A4A44C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x000001852A4A44C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x000001852A4A44C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x000001852A4A44C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x000001852A4A44C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x000001852A4A44C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Baseline: -2917.17 (3256.45) MSE\n",
      "Baseline: -2917.17 (3256.45) MSE\n",
      "Baseline: -2917.17 (3256.45) MSE\n",
      "Baseline: -2917.17 (3256.45) MSE\n",
      "Baseline: -2917.17 (3256.45) MSE\n",
      "Baseline: -2917.17 (3256.45) MSE\n"
     ]
    }
   ],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(1, input_dim=10, activation='relu'))\n",
    "    #model.add(Dense(1))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(1, input_dim=10, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    #model.add(Dense(128, input_dim=10, activation='relu'))\n",
    "    #model.add(Dense(128, activation='relu'))\n",
    "    #model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=5, verbose=0)\n",
    "results = cross_val_score(estimator, features, labels)\n",
    "print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb5395",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
